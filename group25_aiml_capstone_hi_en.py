# -*- coding: utf-8 -*-
"""Group25_AIML_Capstone_HI_EN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gagj_gVh2SwcXolXw4TWTRBUqbVTo837

# **Data Pre-processing**

---
"""

import pandas as pd

import requests

import math
import nltk
nltk.download('punkt_tab')

urls_to_load = [
                'https://raw.githubusercontent.com/gabrielStanovsky/mt_gender/refs/heads/master/data/aggregates/en.txt',
                'https://raw.githubusercontent.com/gabrielStanovsky/mt_gender/refs/heads/master/data/aggregates/en_anti.txt',
                'https://raw.githubusercontent.com/gabrielStanovsky/mt_gender/refs/heads/master/data/aggregates/en_pro.txt'
                ]

#Initializing lists to store the data
en_dataset = []
en_pro_dataset = []
en_anti_dataset = []
en_label = []
en_pro_label = []
en_anti_label = []
en_role = []
en_pro_role = []
en_anti_role = []

#Load datasets

for i, url in enumerate(urls_to_load):

  # URL of the raw text file on GitHub
  url = url

  # Make a GET request to fetch the file content
  response = requests.get(url)

  # Check if the request was successful (status code 200)
  if response.status_code == 200:
    # Get the content of the file as a string
    file_content = response.text
    lines = file_content.splitlines()


    if i == 0:
      for line in lines:
        en_extract = line.split('\t')[2]
        en_dataset.append(en_extract)
        en_label.append(line.split('\t')[0])
        en_role.append(line.split('\t')[3])

    elif i ==1:
      for line in lines:
        en_anti_extract = line.split('\t')[2]
        en_anti_dataset.append(en_anti_extract)
        en_anti_label.append(line.split('\t')[0])
        en_anti_role.append(line.split('\t')[3])

    elif i==2:
      for line in lines:
        en_pro_extract = line.split('\t')[2]
        en_pro_dataset.append(en_pro_extract)
        en_pro_label.append(line.split('\t')[0])
        en_pro_role.append(line.split('\t')[3])

  else:
    print("Failed to retrieve the file. Status code:", response.status_code)

en_role[0:5]

df = pd.DataFrame({'English Sentence': en_dataset, 'Role': en_role, 'Labeled Gender': en_label})
df.head()

df.to_excel('en_dataset.xlsx', index=False)

# Uploading the excel file with Hindi translations generated using Azure Translate API
df2 = pd.read_excel('en_dataset.xlsx')
df2.head()

# Creating train and test set with equal proportion of genders in both sets
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(df2, test_size=0.2, stratify=df2['Labeled Gender'])

train_df = train_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print("Train Set Distribution:\n", train_df['Labeled Gender'].value_counts())
print("\nTest Set Distribution:\n", test_df['Labeled Gender'].value_counts())

test_df.head()

# Extracting sentences, roles and labeled genders in individual lists for further processing
train_en = train_df['English Sentence'].tolist()
test_en = test_df['English Sentence'].tolist()

print(f'Length of English Train set: {len(train_en)}')
print(f'Length of English Test set: {len(test_en)}')

train_hi = train_df['Hindi Sentence'].tolist()
test_hi = test_df['Hindi Sentence'].tolist()

print(f'Length of Hindi Train set: {len(train_hi)}')
print(f'Length of Hindi Test set: {len(test_hi)}')

train_label_en = train_df['Labeled Gender'].tolist()
test_label_en = test_df['Labeled Gender'].tolist()

print(f'Length of Label Train set: {len(train_label_en)}')
print(f'Length of Label Test set: {len(test_label_en)}')

train_role_en = train_df['Role'].tolist()
test_role_en = test_df['Role'].tolist()

print(f'Length of Role Train set: {len(train_role_en)}')
print(f'Length of Role Test set: {len(test_role_en)}')

"""# **Translation from Hindi to English using facebook/mbart model**"""

# Use a pipeline as a high-level helper
from transformers import pipeline

# Create a pipeline for back translation from Hindi to English
pipe_2 = pipeline("translation", model="facebook/mbart-large-50-many-to-many-mmt", src_lang="hi_IN", tgt_lang="en_XX")

# Generate translations from Hindi to English for test set
translations_hi_en = []

translated_sentences = pipe_2(test_hi)

for sentence in translated_sentences:
  translations_hi_en.append(sentence['translation_text'])

# Compute BLEU score between translated text and original English text
from nltk.translate.bleu_score import sentence_bleu

bleu_scores_hi_en = []

for i in range(len(test_en)):
  bleu_scores_hi_en.append((sentence_bleu([test_en[i].split()], translations_hi_en[i].split())))
  if(i % 300 == 0):
    print(i, 'BLEU score -> {}'.format(sentence_bleu([test_en[i].split()], translations_hi_en[i].split())))

# Store translations in an excel file
data = {
    'Hindi Sentence': test_hi,
    'Translated Sentence': translations_hi_en,
    'Original English Sentence': test_en,
    'BLEU Score': bleu_scores_hi_en,
    'Role': test_role_en,
    'Labeled Gender': test_label_en
}
df = pd.DataFrame(data)

df.to_excel('translations_hi_en_1.xlsx', index=False)

"""# **Bias Detection**"""

# Function to evaluate predicted labels
# If sentences has he/him predicted gender is male
# If sentences has she/her predicted gender is female
# If sentence has both he/him and she/her, user is prompted to provide gender
# Default gender is neutral

def evaluate(data, golds):
    labels = []
    neul = 0
    m = 0
    f = 0
    pred_neul = 0
    for i, s in enumerate(data):
        tokens = nltk.word_tokenize(s.lower())

        if 'he' in tokens and 'she' in tokens:
                print(s)
                label = str(input("Enter whether - male or female or neutral : ")) #happens rarely
                if label == 'neutral':
                    pred_neul+=1
                    if golds[i] == 'male':
                        m+=1
                    elif golds[i] == 'female':
                        f+=1
                    elif golds[i] == 'neutral':
                        neul+=1

                labels.append(label)

        elif 'she' in tokens:
            labels.append('female')

        elif 'he' in tokens:
            labels.append('male')

        elif 'his' in tokens or 'him' in tokens:
            if 'her' in tokens:
                print("Mixed : ",i+1 ,s)
                label = str(input("Enter whether - male or female : ")) #happens rarely
                labels.append(label)
                if golds[i] == 'male':
                  m+=1
                elif golds[i] == 'female':
                  f+=1
                elif golds[i] == 'neutral':
                  neul+=1

            else:
                labels.append('male')
                continue

        elif 'her' in tokens:
            labels.append('female')
            continue

        else:
            labels.append('neutral')
            pred_neul+=1
            if golds[i] == 'male':
              m+=1
            elif golds[i] == 'female':
              f+=1
            elif golds[i] == 'neutral':
              neul+=1


    return labels

# Load the translations
df1 = pd.read_excel('translations_hi_en_1.xlsx')
df1.head()

df1.shape

# Filtering data where BLEU score is less than 10^-200 to remove hallucinations
df1 = df1[df1['BLEU Score'] > 1e-200]
df1.head()

df1.shape

target_sentence = df1['Translated Sentence']
target_sentences = target_sentence.to_list()

# Filtering data by removing filtered indexes
filtered_translations_en_hi = [test_hi[i] for i in df1.index]
filtered_translations_hi_en = [translations_hi_en[i] for i in df1.index]
filtered_test_en = [test_en[i] for i in df1.index]
filtered_test_label_en = [test_label_en[i] for i in df1.index]
filtered_test_role_en = [test_role_en[i] for i in df1.index]
filtered_bleu_scores_hi_en = [bleu_scores_hi_en[i] for i in df1.index]
filtered_test_role_en = [test_role_en[i] for i in df1.index]

# Predicting gender from translated English sentences
predicted_gender = evaluate(target_sentences, filtered_test_label_en)

# Calculate accuracy score between predicted gender and labeled gender
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(filtered_test_label_en, predicted_gender)
print("acc percentage")
print(accuracy*100)

# Calculate precision score between predicted gender and labeled gender
from sklearn.metrics import precision_score

precision = precision_score(filtered_test_label_en, predicted_gender, average='macro')
print(f'Precision: {precision*100}')

# Calculate recall and f1 score between predicted gender and labeled gender
from sklearn.metrics import recall_score, f1_score

recall = recall_score(filtered_test_label_en, predicted_gender, average='macro')
print(f'Recall: {recall*100}')

f1 = f1_score(filtered_test_label_en, predicted_gender, average='macro')
print(f'F1 Score: {f1*100}')

# Generate Confusion matrix

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'filtered_test_label_en' and 'predicted_gender' are defined as in the original code
cm = confusion_matrix(filtered_test_label_en, predicted_gender, labels=['male', 'female', 'neutral'])

# Plot the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['male', 'female', 'neutral'],
            yticklabels=['male', 'female', 'neutral'])
plt.xlabel('Predicted Gender')
plt.ylabel('True Gender')
plt.title('Confusion Matrix')
plt.show()

# Store final translations, original sentence and BLEU score in excel file
data = {
    'Hindi Sentence': filtered_translations_en_hi,
    'Translated Sentence': filtered_translations_hi_en,
    'Original English Sentence': filtered_test_en,
    'BLEU Score': filtered_bleu_scores_hi_en,
    'Role': filtered_test_role_en,
    'Labeled Gender': filtered_test_label_en,
    'Predicted Gender': predicted_gender
}
df = pd.DataFrame(data)

df.to_excel('translations_hi_en_2.xlsx', index=False)

"""# **AIF 360 - Measure Bias using DI and SPD**"""

!pip install aif360 aif360[Reductions] aif360[inFairness]

# Import necessary libraries
import pandas as pd
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.datasets import StructuredDataset
from aif360.datasets import StandardDataset

# Load your dataset
file_path = "/content/translations_hi_en_2.xlsx"  # Replace with your file path - run once for base model output and fine-tuned model output
data = pd.read_excel(file_path)

# Map gender to numerical values: male=0, female=1, neutral=2
gender_mapping = {'male': 0, 'female': 1}
data['labeled_gender_numeric'] = data['Labeled Gender'].map(gender_mapping)
data['predicted_gender_numeric'] = data['Predicted Gender'].map(gender_mapping)
data.head()

# Ensure data doesn't have NaN or invalid values
data = data.dropna(subset=['labeled_gender_numeric', 'predicted_gender_numeric'])

# Ensure DataFrame contains only numeric values
numeric_columns = ['labeled_gender_numeric', 'predicted_gender_numeric']
data = data[numeric_columns]  # Keep only numeric columns

# Create the StandardDataset
standard_dataset = StandardDataset(
    df=data,
    label_name='predicted_gender_numeric',
    protected_attribute_names=['labeled_gender_numeric'],
    favorable_classes=[1],
    privileged_classes=[[1]],
    metadata={
        'label_maps': [{0: 'male', 1: 'female'}],
        'protected_attribute_maps': [{0: 'male', 1: 'female'}]
    }
)

# Define the classes
classes = {'male': 0, 'female': 1}

# Perform pairwise comparisons for each privileged and unprivileged group
for privileged_name, privileged_value in classes.items():
    for unprivileged_name, unprivileged_value in classes.items():
        if privileged_value != unprivileged_value:
            # Define privileged and unprivileged groups using dictionaries
            privileged_groups = [{'labeled_gender_numeric': privileged_value}]
            unprivileged_groups = [{'labeled_gender_numeric': unprivileged_value}]

            # Compute metrics
            metric = ClassificationMetric(
                standard_dataset,
                standard_dataset,  # Assumes true and predicted labels are in the same dataset
                privileged_groups=privileged_groups,
                unprivileged_groups=unprivileged_groups
            )

            # Calculate and print metrics
            spd = metric.statistical_parity_difference()
            di = metric.disparate_impact()


            print(f"Metrics for {privileged_name} (privileged) vs {unprivileged_name} (unprivileged):")
            print(f"  Statistical Parity Difference (SPD): {spd}")
            print(f"  Disparate Impact (DI): {di}")

"""**The model is biased towards males:**

The Statistical Parity Difference (SPD) scores are positive when comparing males to females, indicating that the model is biased towards males.

The Disparate Impact (DI) scores are greater than 1 when comparing males to females, indicating that the model is biased towards males.

**The model is not biased when comparing males to neutral:**

The SPD score is close to 0 when comparing males to neutral, indicating that the model is not biased towards or against males when comparing to neutral.

The DI score is 0 when comparing males to neutral, indicating that the model is fair when comparing males to neutral.

**The model is biased against females:**

The SPD scores are negative when comparing females to males, indicating that the model is biased against females.
The DI scores are less than 1 when comparing females to males, indicating that the model is biased against females.

**The model is biased towards neutral:**

The SPD scores are positive when comparing neutral to males and females, indicating that the model is biased towards neutral.
The DI scores are infinite when comparing neutral to males and females, indicating that the model is highly biased towards neutral.

# **Bias Mitigation**

In this section we will mitigate bias using fine tuning. For fine tuning, we require language pair train set where source language is Hindi and target language is English.
"""

# Store translations to as train set
data = {
    'Source Hindi Sentence': train_hi,
    'Target English Sentence': train_en,
    'Role': train_role_en,
    'Labeled Gender': train_label_en
}

df = pd.DataFrame(data)

df.to_excel('train_set_en_hi.xlsx', index=False)

# Fine tuning facebook/mbart model
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

# Load train set

df = pd.read_excel("train_set_en_hi.xlsx")

hi_source_sentences = df['Source Hindi Sentence'].tolist()  # Hindi sentences
en_target_sentences = df['Target English Sentence'].tolist()  # English sentences

!pip install datasets

# Create a dataset-like format using Hugging Face Datasets
from datasets import Dataset

data = {
    "source": hi_source_sentences,
    "target": en_target_sentences
}
dataset = Dataset.from_dict(data)

from sklearn.model_selection import train_test_split

# Split the dataset into train and validation sets (e.g., 80% train, 20% validation)
split_dataset = dataset.train_test_split(test_size=0.2, seed=42)

# Extract the train and validation datasets
train_dataset = split_dataset['train']
val_dataset = split_dataset['test']

# Tokenize the data
def tokenize_function(examples):
    inputs = tokenizer(examples['source'], padding="max_length", truncation=True, max_length=128, return_tensors="pt")
    targets = tokenizer(examples['target'], padding="max_length", truncation=True, max_length=128, return_tensors="pt")

    # Add the target labels (target sentences)
    inputs["labels"] = targets["input_ids"]

    return inputs

# Map tokenization to the dataset
train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)
val_tokenized_datasets = val_dataset.map(tokenize_function, batched=True)

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
# Set up training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none",
    gradient_accumulation_steps=2
)

# Set up the trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized_datasets,
    eval_dataset=val_tokenized_datasets
)

# Train the model
trainer.train()

model_path = "./fine_tuned_model"

# Save the model

model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

# Initializing list to store Hindi to English translations from fine-tuned model
ft_translations_hi_en = []

# Load the fine tuned model and run it against test set
fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
fine_tuned_tokenizer = AutoTokenizer.from_pretrained(model_path)

fine_tuned_model.eval()

# Tokenize the test sentences
inputs = fine_tuned_tokenizer(test_hi, return_tensors="pt", padding=True, truncation=True)

# Generate the translations (translate from Hindi to English)
generated_ids = fine_tuned_model.generate(
    inputs["input_ids"],
    forced_bos_token_id=fine_tuned_tokenizer.lang_code_to_id["en_XX"],
)

# Decode the generated token IDs back to text
translations = fine_tuned_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

ft_translations_hi_en.extend(translations)

len(ft_translations_hi_en)

# Compute BLEU score between translated text and original English text
from nltk.translate.bleu_score import sentence_bleu

ft_bleu_scores_hi_en = []

for i in range(len(test_en)):
  ft_bleu_scores_hi_en.append((sentence_bleu([test_en[i].split()], ft_translations_hi_en[i].split())))

# Store translations in an excel file
data = {
    'Hindi Sentence': test_hi,
    'Translated Sentence': ft_translations_hi_en,
    'Original English Sentence': test_en,
    'BLEU Score': ft_bleu_scores_hi_en,
    'Role': test_role_en,
    'Labeled Gender': test_label_en
}
df = pd.DataFrame(data)

df.to_excel('ft_translations_hi_en_1.xlsx', index=False)

df1 = pd.read_excel('ft_translations_hi_en_1.xlsx')
df1.head()

df1.shape

# Filtering data where BLEU score is less than 10^-200 to remove hallucinations
df1 = df1[df1['BLEU Score'] > 1e-200]
df1.head()

df1.shape

target_sentence = df1['Translated Sentence']
target_sentences = target_sentence.to_list()

# Filtering data by removing filtered indexes
filtered_translations_en_hi = [test_hi[i] for i in df1.index]
filtered_translations_hi_en = [ft_translations_hi_en[i] for i in df1.index]
filtered_test_en = [test_en[i] for i in df1.index]
filtered_test_label_en = [test_label_en[i] for i in df1.index]
filtered_test_role_en = [test_role_en[i] for i in df1.index]
filtered_bleu_scores_hi_en = [ft_bleu_scores_hi_en[i] for i in df1.index]
filtered_test_role_en = [test_role_en[i] for i in df1.index]

# Predict the gender from translated English sentences
predicted_gender = evaluate(target_sentences, filtered_test_label_en)

# Store final translations, original sentence and BLEU score in excel file
data = {
    'Hindi Sentence': filtered_translations_en_hi,
    'Translated Sentence': filtered_translations_hi_en,
    'Original English Sentence': filtered_test_en,
    'BLEU Score': filtered_bleu_scores_hi_en,
    'Role': filtered_test_role_en,
    'Labeled Gender': filtered_test_label_en,
    'Predicted Gender': predicted_gender
}
df = pd.DataFrame(data)

df.to_excel('ft_translations_hi_en_2.xlsx', index=False)

# Calculate accuracy score of fine-tuned model

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(filtered_test_label_en, predicted_gender)
print("acc percentage")
print(accuracy*100)

# Calculate precision score of fine-tuned model
from sklearn.metrics import precision_score

precision = precision_score(filtered_test_label_en, predicted_gender, average='macro')
print(f'Precision: {precision*100}')

# Calculate recall and f1 score of fine-tuned model
from sklearn.metrics import recall_score, f1_score

recall = recall_score(filtered_test_label_en, predicted_gender, average='macro')
print(f'Recall (Macro Average): {recall*100}')

f1 = f1_score(filtered_test_label_en, predicted_gender, average='macro')
print(f'F1 Score (Macro Average): {f1*100}')

# Create Confusion Matrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'filtered_test_label_en' and 'predicted_gender' are defined as in the original code
cm = confusion_matrix(filtered_test_label_en, predicted_gender, labels=['male', 'female', 'neutral'])

# Plot the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['male', 'female', 'neutral'],
            yticklabels=['male', 'female', 'neutral'])
plt.xlabel('Predicted Gender')
plt.ylabel('True Gender')
plt.title('Confusion Matrix')
plt.show()

"""For AI Fairness calculation, use the code block in *AIF 360 - Measure Bias using DI and SPD* section and replace the file to fine-tuned model output"""